{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897fd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements here\n",
    "\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1ea2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 02:50:33.192735: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-22 02:50:35.121419: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "2026-01-22 02:50:41.823103: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5651b",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The codes below trains a simple deep learning model to recognize hand-written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deac4d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Training data shape: (60000, 28, 28), Labels shape: (60000,)\n",
      "First 10 labels in y_train: [5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}, Labels shape: {y_train.shape}\")\n",
    "print(\"First 10 labels in y_train:\", y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f30ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype(\"float32\") / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff7eac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "]) \n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6c6605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 48ms/step - accuracy: 0.8776 - loss: 0.3975 - val_accuracy: 0.9801 - val_loss: 0.0629\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 39ms/step - accuracy: 0.9649 - loss: 0.1196 - val_accuracy: 0.9863 - val_loss: 0.0433\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9769 - loss: 0.0801 - val_accuracy: 0.9867 - val_loss: 0.0369\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.9825 - loss: 0.0631 - val_accuracy: 0.9893 - val_loss: 0.0306\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9838 - loss: 0.0547 - val_accuracy: 0.9919 - val_loss: 0.0230\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 46ms/step - accuracy: 0.9862 - loss: 0.0463 - val_accuracy: 0.9919 - val_loss: 0.0244\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.9883 - loss: 0.0417 - val_accuracy: 0.9906 - val_loss: 0.0279\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 47ms/step - accuracy: 0.9888 - loss: 0.0373 - val_accuracy: 0.9920 - val_loss: 0.0239\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 44ms/step - accuracy: 0.9903 - loss: 0.0327 - val_accuracy: 0.9911 - val_loss: 0.0302\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 41ms/step - accuracy: 0.9913 - loss: 0.0311 - val_accuracy: 0.9914 - val_loss: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3256e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9914 - loss: 0.0295\n",
      "Test loss: 0.029462426900863647, Test accuracy: 0.9914000034332275\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss}, Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1ac870",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "\n",
    "Now that we have trained a model, let's use this model to process a hand-written digit image and make inference.\n",
    "Let's take a few images from `x_test`. \n",
    "\n",
    "Note that each image should be of shape `(28, 28, 1)`, and the values\n",
    "should be normalized (divided by `255.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d0db57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x76ec5760bc90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGqhJREFUeJzt3X9sVfX9x/FXi/SC2l4spb29o0BBBcMvJ4Pa8GMoDbQuBrRLQP0DFgKBXcyw88e6iChb0o0ljrgg/rPATMRfiUAkSzMptoTZYqgwwqYd7boBgRbFcW8pUhj9fP8g3q9XCnjKvX33Xp6P5CT03vPpfXs84clpb0/TnHNOAAD0sXTrAQAANycCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATNxiPcC3dXd368SJE8rMzFRaWpr1OAAAj5xz6ujoUDAYVHr61a9z+l2ATpw4oYKCAusxAAA36NixYxo+fPhVn+93X4LLzMy0HgEAEAfX+/s8YQHauHGjRo0apUGDBqmoqEgff/zxd1rHl90AIDVc7+/zhATo7bffVkVFhdauXatPPvlEkydP1rx583Tq1KlEvBwAIBm5BJg2bZoLhULRjy9duuSCwaCrqqq67tpwOOwksbGxsbEl+RYOh6/5933cr4AuXLigxsZGlZSURB9LT09XSUmJ6uvrr9i/q6tLkUgkZgMApL64B+iLL77QpUuXlJeXF/N4Xl6e2trarti/qqpKfr8/uvEOOAC4OZi/C66yslLhcDi6HTt2zHokAEAfiPvPAeXk5GjAgAFqb2+Peby9vV2BQOCK/X0+n3w+X7zHAAD0c3G/AsrIyNCUKVNUU1MTfay7u1s1NTUqLi6O98sBAJJUQu6EUFFRocWLF+sHP/iBpk2bpg0bNqizs1M/+clPEvFyAIAklJAALVy4UJ9//rleeOEFtbW16d5771V1dfUVb0wAANy80pxzznqIb4pEIvL7/dZjAABuUDgcVlZW1lWfN38XHADg5kSAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEzEPUAvvvii0tLSYrZx48bF+2UAAEnulkR80vHjx2vXrl3//yK3JORlAABJLCFluOWWWxQIBBLxqQEAKSIh3wM6cuSIgsGgRo8erSeeeEJHjx696r5dXV2KRCIxGwAg9cU9QEVFRdqyZYuqq6u1adMmtba2aubMmero6Ohx/6qqKvn9/uhWUFAQ75EAAP1QmnPOJfIFzpw5o5EjR+rll1/W0qVLr3i+q6tLXV1d0Y8jkQgRAoAUEA6HlZWVddXnE/7ugCFDhujuu+9Wc3Nzj8/7fD75fL5EjwEA6GcS/nNAZ8+eVUtLi/Lz8xP9UgCAJBL3AD399NOqq6vTv//9b3300Ud65JFHNGDAAD322GPxfikAQBKL+5fgjh8/rscee0ynT5/WsGHDNGPGDDU0NGjYsGHxfikAQBJL+JsQvIpEIvL7/dZjAABu0PXehMC94AAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwn/hXToWz/+8Y89r1m2bFmvXuvEiROe15w/f97zmjfeeMPzmra2Ns9rJF31FycCiD+ugAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGAizTnnrIf4pkgkIr/fbz1G0vrXv/7lec2oUaPiP4ixjo6OXq37+9//HudJEG/Hjx/3vGb9+vW9eq39+/f3ah0uC4fDysrKuurzXAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACZusR4A8bVs2TLPayZNmtSr1/r00089r7nnnns8r7nvvvs8r5k9e7bnNZJ0//33e15z7Ngxz2sKCgo8r+lL//vf/zyv+fzzzz2vyc/P97ymN44ePdqrddyMNLG4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAz0hRTU1PTJ2t6q7q6uk9e54477ujVunvvvdfzmsbGRs9rpk6d6nlNXzp//rznNf/85z89r+nNDW2zs7M9r2lpafG8BonHFRAAwAQBAgCY8BygPXv26OGHH1YwGFRaWpq2b98e87xzTi+88ILy8/M1ePBglZSU6MiRI/GaFwCQIjwHqLOzU5MnT9bGjRt7fH79+vV65ZVX9Nprr2nfvn267bbbNG/evF59TRkAkLo8vwmhrKxMZWVlPT7nnNOGDRv0/PPPa/78+ZKk119/XXl5edq+fbsWLVp0Y9MCAFJGXL8H1Nraqra2NpWUlEQf8/v9KioqUn19fY9rurq6FIlEYjYAQOqLa4Da2tokSXl5eTGP5+XlRZ/7tqqqKvn9/uhWUFAQz5EAAP2U+bvgKisrFQ6Ho9uxY8esRwIA9IG4BigQCEiS2tvbYx5vb2+PPvdtPp9PWVlZMRsAIPXFNUCFhYUKBAIxP1kfiUS0b98+FRcXx/OlAABJzvO74M6ePavm5ubox62trTp48KCys7M1YsQIrV69Wr/+9a911113qbCwUGvWrFEwGNSCBQviOTcAIMl5DtD+/fv1wAMPRD+uqKiQJC1evFhbtmzRs88+q87OTi1fvlxnzpzRjBkzVF1drUGDBsVvagBA0ktzzjnrIb4pEonI7/dbjwHAo/Lycs9r3nnnHc9rDh8+7HnNN//R7MWXX37Zq3W4LBwOX/P7+ubvggMA3JwIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwvOvYwCQ+nJzcz2vefXVVz2vSU/3/m/gdevWeV7DXa37J66AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3IwUwBVCoZDnNcOGDfO85r///a/nNU1NTZ7XoH/iCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSIEUNn369F6t+8UvfhHnSXq2YMECz2sOHz4c/0FggisgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyMFUthDDz3Uq3UDBw70vKampsbzmvr6es9rkDq4AgIAmCBAAAATngO0Z88ePfzwwwoGg0pLS9P27dtjnl+yZInS0tJittLS0njNCwBIEZ4D1NnZqcmTJ2vjxo1X3ae0tFQnT56Mbm+++eYNDQkASD2e34RQVlamsrKya+7j8/kUCAR6PRQAIPUl5HtAtbW1ys3N1dixY7Vy5UqdPn36qvt2dXUpEonEbACA1Bf3AJWWlur1119XTU2Nfvvb36qurk5lZWW6dOlSj/tXVVXJ7/dHt4KCgniPBADoh+L+c0CLFi2K/nnixImaNGmSxowZo9raWs2ZM+eK/SsrK1VRURH9OBKJECEAuAkk/G3Yo0ePVk5Ojpqbm3t83ufzKSsrK2YDAKS+hAfo+PHjOn36tPLz8xP9UgCAJOL5S3Bnz56NuZppbW3VwYMHlZ2drezsbL300ksqLy9XIBBQS0uLnn32Wd15552aN29eXAcHACQ3zwHav3+/HnjggejHX3//ZvHixdq0aZMOHTqkP/3pTzpz5oyCwaDmzp2rX/3qV/L5fPGbGgCQ9NKcc856iG+KRCLy+/3WYwD9zuDBgz2v2bt3b69ea/z48Z7XPPjgg57XfPTRR57XIHmEw+Frfl+fe8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARNx/JTeAxHjmmWc8r/n+97/fq9eqrq72vIY7W8MrroAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBQw8KMf/cjzmjVr1nheE4lEPK+RpHXr1vVqHeAFV0AAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAluRgrcoKFDh3pe88orr3heM2DAAM9r/vznP3teI0kNDQ29Wgd4wRUQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCm5EC39CbG35WV1d7XlNYWOh5TUtLi+c1a9as8bwG6CtcAQEATBAgAIAJTwGqqqrS1KlTlZmZqdzcXC1YsEBNTU0x+5w/f16hUEhDhw7V7bffrvLycrW3t8d1aABA8vMUoLq6OoVCITU0NOiDDz7QxYsXNXfuXHV2dkb3eeqpp/T+++/r3XffVV1dnU6cOKFHH3007oMDAJKbpzchfPubrVu2bFFubq4aGxs1a9YshcNh/fGPf9TWrVv14IMPSpI2b96se+65Rw0NDbr//vvjNzkAIKnd0PeAwuGwJCk7O1uS1NjYqIsXL6qkpCS6z7hx4zRixAjV19f3+Dm6uroUiURiNgBA6ut1gLq7u7V69WpNnz5dEyZMkCS1tbUpIyNDQ4YMidk3Ly9PbW1tPX6eqqoq+f3+6FZQUNDbkQAASaTXAQqFQjp8+LDeeuutGxqgsrJS4XA4uh07duyGPh8AIDn06gdRV61apZ07d2rPnj0aPnx49PFAIKALFy7ozJkzMVdB7e3tCgQCPX4un88nn8/XmzEAAEnM0xWQc06rVq3Stm3btHv37it+mnvKlCkaOHCgampqoo81NTXp6NGjKi4ujs/EAICU4OkKKBQKaevWrdqxY4cyMzOj39fx+/0aPHiw/H6/li5dqoqKCmVnZysrK0tPPvmkiouLeQccACCGpwBt2rRJkjR79uyYxzdv3qwlS5ZIkn7/+98rPT1d5eXl6urq0rx58/Tqq6/GZVgAQOpIc8456yG+KRKJyO/3W4+Bm9Tdd9/tec1nn32WgEmuNH/+fM9r3n///QRMAnw34XBYWVlZV32ee8EBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARK9+IyrQ340cObJX6/7yl7/EeZKePfPMM57X7Ny5MwGTAHa4AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATHAzUqSk5cuX92rdiBEj4jxJz+rq6jyvcc4lYBLADldAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJbkaKfm/GjBme1zz55JMJmARAPHEFBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY4Gak6Pdmzpzpec3tt9+egEl61tLS4nnN2bNnEzAJkFy4AgIAmCBAAAATngJUVVWlqVOnKjMzU7m5uVqwYIGamppi9pk9e7bS0tJithUrVsR1aABA8vMUoLq6OoVCITU0NOiDDz7QxYsXNXfuXHV2dsbst2zZMp08eTK6rV+/Pq5DAwCSn6c3IVRXV8d8vGXLFuXm5qqxsVGzZs2KPn7rrbcqEAjEZ0IAQEq6oe8BhcNhSVJ2dnbM42+88YZycnI0YcIEVVZW6ty5c1f9HF1dXYpEIjEbACD19fpt2N3d3Vq9erWmT5+uCRMmRB9//PHHNXLkSAWDQR06dEjPPfecmpqa9N577/X4eaqqqvTSSy/1dgwAQJLqdYBCoZAOHz6svXv3xjy+fPny6J8nTpyo/Px8zZkzRy0tLRozZswVn6eyslIVFRXRjyORiAoKCno7FgAgSfQqQKtWrdLOnTu1Z88eDR8+/Jr7FhUVSZKam5t7DJDP55PP5+vNGACAJOYpQM45Pfnkk9q2bZtqa2tVWFh43TUHDx6UJOXn5/dqQABAavIUoFAopK1bt2rHjh3KzMxUW1ubJMnv92vw4MFqaWnR1q1b9dBDD2no0KE6dOiQnnrqKc2aNUuTJk1KyH8AACA5eQrQpk2bJF3+YdNv2rx5s5YsWaKMjAzt2rVLGzZsUGdnpwoKClReXq7nn38+bgMDAFKD5y/BXUtBQYHq6upuaCAAwM2Bu2ED3/C3v/3N85o5c+Z4XvPll196XgOkGm5GCgAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYSHPXu8V1H4tEIvL7/dZjAABuUDgcVlZW1lWf5woIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACAiX4XoH52azoAQC9d7+/zfhegjo4O6xEAAHFwvb/P+93dsLu7u3XixAllZmYqLS0t5rlIJKKCggIdO3bsmndYTXUch8s4DpdxHC7jOFzWH46Dc04dHR0KBoNKT7/6dc4tfTjTd5Kenq7hw4dfc5+srKyb+gT7GsfhMo7DZRyHyzgOl1kfh+/ya3X63ZfgAAA3BwIEADCRVAHy+Xxau3atfD6f9SimOA6XcRwu4zhcxnG4LJmOQ797EwIA4OaQVFdAAIDUQYAAACYIEADABAECAJhImgBt3LhRo0aN0qBBg1RUVKSPP/7YeqQ+9+KLLyotLS1mGzdunPVYCbdnzx49/PDDCgaDSktL0/bt22Oed87phRdeUH5+vgYPHqySkhIdOXLEZtgEut5xWLJkyRXnR2lpqc2wCVJVVaWpU6cqMzNTubm5WrBggZqammL2OX/+vEKhkIYOHarbb79d5eXlam9vN5o4Mb7LcZg9e/YV58OKFSuMJu5ZUgTo7bffVkVFhdauXatPPvlEkydP1rx583Tq1Cnr0frc+PHjdfLkyei2d+9e65ESrrOzU5MnT9bGjRt7fH79+vV65ZVX9Nprr2nfvn267bbbNG/ePJ0/f76PJ02s6x0HSSotLY05P958880+nDDx6urqFAqF1NDQoA8++EAXL17U3Llz1dnZGd3nqaee0vvvv693331XdXV1OnHihB599FHDqePvuxwHSVq2bFnM+bB+/Xqjia/CJYFp06a5UCgU/fjSpUsuGAy6qqoqw6n63tq1a93kyZOtxzAlyW3bti36cXd3twsEAu53v/td9LEzZ844n8/n3nzzTYMJ+8a3j4Nzzi1evNjNnz/fZB4rp06dcpJcXV2dc+7y//uBAwe6d999N7rPp59+6iS5+vp6qzET7tvHwTnnfvjDH7qf/exndkN9B/3+CujChQtqbGxUSUlJ9LH09HSVlJSovr7ecDIbR44cUTAY1OjRo/XEE0/o6NGj1iOZam1tVVtbW8z54ff7VVRUdFOeH7W1tcrNzdXYsWO1cuVKnT592nqkhAqHw5Kk7OxsSVJjY6MuXrwYcz6MGzdOI0aMSOnz4dvH4WtvvPGGcnJyNGHCBFVWVurcuXMW411Vv7sZ6bd98cUXunTpkvLy8mIez8vL02effWY0lY2ioiJt2bJFY8eO1cmTJ/XSSy9p5syZOnz4sDIzM63HM9HW1iZJPZ4fXz93sygtLdWjjz6qwsJCtbS06Je//KXKyspUX1+vAQMGWI8Xd93d3Vq9erWmT5+uCRMmSLp8PmRkZGjIkCEx+6by+dDTcZCkxx9/XCNHjlQwGNShQ4f03HPPqampSe+9957htLH6fYDw/8rKyqJ/njRpkoqKijRy5Ei98847Wrp0qeFk6A8WLVoU/fPEiRM1adIkjRkzRrW1tZozZ47hZIkRCoV0+PDhm+L7oNdyteOwfPny6J8nTpyo/Px8zZkzRy0tLRozZkxfj9mjfv8luJycHA0YMOCKd7G0t7crEAgYTdU/DBkyRHfffbeam5utRzHz9TnA+XGl0aNHKycnJyXPj1WrVmnnzp368MMPY359SyAQ0IULF3TmzJmY/VP1fLjacehJUVGRJPWr86HfBygjI0NTpkxRTU1N9LHu7m7V1NSouLjYcDJ7Z8+eVUtLi/Lz861HMVNYWKhAIBBzfkQiEe3bt++mPz+OHz+u06dPp9T54ZzTqlWrtG3bNu3evVuFhYUxz0+ZMkUDBw6MOR+ampp09OjRlDofrnccenLw4EFJ6l/ng/W7IL6Lt956y/l8Prdlyxb3j3/8wy1fvtwNGTLEtbW1WY/Wp37+85+72tpa19ra6v7617+6kpISl5OT406dOmU9WkJ1dHS4AwcOuAMHDjhJ7uWXX3YHDhxw//nPf5xzzv3mN79xQ4YMcTt27HCHDh1y8+fPd4WFhe6rr74ynjy+rnUcOjo63NNPP+3q6+tda2ur27Vrl7vvvvvcXXfd5c6fP289etysXLnS+f1+V1tb606ePBndzp07F91nxYoVbsSIEW737t1u//79rri42BUXFxtOHX/XOw7Nzc1u3bp1bv/+/a61tdXt2LHDjR492s2aNct48lhJESDnnPvDH/7gRowY4TIyMty0adNcQ0OD9Uh9buHChS4/P99lZGS4733ve27hwoWuubnZeqyE+/DDD52kK7bFixc75y6/FXvNmjUuLy/P+Xw+N2fOHNfU1GQ7dAJc6zicO3fOzZ071w0bNswNHDjQjRw50i1btizl/pHW03+/JLd58+boPl999ZX76U9/6u644w536623ukceecSdPHnSbugEuN5xOHr0qJs1a5bLzs52Pp/P3Xnnne6ZZ55x4XDYdvBv4dcxAABM9PvvAQEAUhMBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYOL/AI1ahUakGRHyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image1 = x_test[0].reshape(28, 28, 1) # Note the shape is (28, 28, 1) for a single image.\n",
    "image2 = x_test[1].reshape(28, 28, 1) # Note the shape is (28, 28, 1) for a single image.\n",
    "\n",
    "# Display the first image in the test set\n",
    "plt.imshow(image1[:, :, 0], cmap='gray') # change \"image1\" to \"image2\" to display the second image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532b071",
   "metadata": {},
   "source": [
    "To make predictions, we need to make a list of multiple images (each image of shape `(28, 28, 1)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbcd3bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "[[5.1030818e-11 1.1866126e-07 1.5265051e-08 4.9132548e-10 2.9154888e-08\n",
      "  4.0653492e-12 2.6554053e-14 9.9999988e-01 5.4810223e-12 2.4531162e-09]\n",
      " [6.5861374e-09 2.8533771e-09 1.0000000e+00 3.2211022e-11 2.9502116e-11\n",
      "  1.1746753e-15 6.6485888e-09 4.4606489e-12 5.3750289e-09 1.5529754e-14]]\n",
      "Predicted classes: [7 2]\n"
     ]
    }
   ],
   "source": [
    "image_list = [image1, image2]\n",
    "\n",
    "predicted_res = model.predict(np.array(image_list))\n",
    "print(predicted_res)\n",
    "\n",
    "# The predicted class for each image\n",
    "predicted_classes = np.argmax(predicted_res, axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48247fe3",
   "metadata": {},
   "source": [
    "## Remember!\n",
    "\n",
    "what's important here is that the data sent for prediction is an array (or list) or one or multiple images, each of shape `(28, 28, 1)`.\n",
    "\n",
    "Therefore, the shape of the input data is `(n, 28, 28, 1)` where `n` is the number of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee86738",
   "metadata": {},
   "source": [
    "# Save the model into a folder\n",
    "\n",
    "The following cell saves the model into a folder, so that it can be loaded by `Tensorflow Serving` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03622d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/digit_classifier/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/digit_classifier/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'models/digit_classifier/1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  130757450719376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450720144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450719760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450722256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450721488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450723216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450722640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450722832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450721872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  130757450723024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "# Here `1` is the version number of the model.\n",
    "# You can change it to any number you like, but it should be unique for each version.\n",
    "model_path = \"models/digit_classifier/1\"\n",
    "\n",
    "# Delete the model directory if it exists\n",
    "import os\n",
    "if os.path.exists(model_path):\n",
    "    import shutil\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "# Export the model\n",
    "model.export(\n",
    "    model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
